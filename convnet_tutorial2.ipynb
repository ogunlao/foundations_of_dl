{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of convnet_tutorial2.ipynb","provenance":[{"file_id":"https://github.com/lvdmaaten/convnet_tutorials/blob/master/convnet_tutorial2.ipynb","timestamp":1587598181664},{"file_id":"1Eq_v8eTk4XAxFXhjL6rk26gwfwvpZYBn","timestamp":1553633151446}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mCt-frpFH8oN"},"source":["The tutorials use PyTorch. You will need to load the following dependencies."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hqnl0AKVXIA4","colab":{}},"source":["import random\n","\n","import PIL\n","import imageio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import skimage.transform\n","import torch\n","import torch.nn as nn\n","import torch.utils.data\n","import torchvision\n","from torchvision import datasets, transforms\n","from IPython import display"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CKeYuM-cIXxs"},"source":["The code below may be helpful in visualizing PyTorch tensors as images."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZZd_rI8edYIB","colab":{}},"source":["%matplotlib inline\n","\n","def show(img):\n","    \"\"\"Show PyTorch tensor img as an image in matplotlib.\"\"\"\n","    npimg = img.cpu().detach().numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n","    plt.grid(False)\n","    plt.gca().axis('off')\n","\n","def display_thumb(img):\n","  display.display(transforms.Resize(128)(img))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dzfEE578uSNp","colab":{}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zgE0byUgKwM6"},"source":["Load MNIST and define train/test functions as before. Please make sure you read the code carefully and understand what it is doing."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-NMUce6PKu-o","colab":{}},"source":["# Load the training and test dataset.\n","mnist_train = datasets.MNIST('/tmp/mnist', train=True, download=True, transform=transforms.ToTensor())\n","mnist_test = datasets.MNIST('/tmp/mnist', train=False, download=True, transform=transforms.ToTensor())\n","\n","# Size of the batches the data loader will produce.\n","batch_size = 64\n","\n","# This creates the dataloaders.\n","train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TgAJ94UgK1VU","colab":{}},"source":["def train(model, criterion, data_loader, optimizer, num_epochs):\n","    \"\"\"Simple training loop for a PyTorch model.\"\"\" \n","    \n","    # Make sure model is in training mode.\n","    model.train()\n","    \n","    # Move model to the device (CPU or GPU).\n","    model.to(device)\n","    \n","    # Exponential moving average of the loss.\n","    ema_loss = None\n","    \n","    # Loop over epochs.\n","    for epoch in range(num_epochs):\n","        \n","      # Loop over data.\n","      for batch_idx, (data, target) in enumerate(data_loader):\n","            \n","          # Forward pass.\n","          output = model(data.to(device))\n","          loss = criterion(output.to(device), target.to(device))\n","          \n","          # Backward pass.\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","          \n","          # NOTE: It is important to call .item() on the loss before summing.\n","          if ema_loss is None:\n","            ema_loss = loss.item()\n","          else:\n","            ema_loss += (loss.item() - ema_loss) * 0.01 \n","          \n","      # Print out progress the end of epoch.\n","      print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n","            epoch, ema_loss),\n","      )\n","              \n","              \n","def test(model, data_loader):\n","    \"\"\"Measures the accuracy of a model on a data set.\"\"\" \n","    # Make sure the model is in evaluation mode.\n","    model.eval()\n","    correct = 0\n","\n","    # We do not need to maintain intermediate activations while testing.\n","    with torch.no_grad():\n","        \n","        # Loop over test data.\n","        for data, target in data_loader:\n","          \n","            # Forward pass.\n","            output = model(data.to(device))\n","            \n","            # Get the label corresponding to the highest predicted probability.\n","            pred = output.argmax(dim=1, keepdim=True)\n","            \n","            # Count number of correct predictions.\n","            correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n","\n","    # Print test accuracy.\n","    percent = 100. * correct / len(data_loader.dataset)\n","    print(f'Accuracy: {correct} / {len(data_loader.dataset)} ({percent:.0f}%)')\n","    return percent\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d7KaK3wAOY37","colab_type":"text"},"source":["In the last tutorial, you implemented a naive convolution. In this section you will implement your own version of forward pass of nn.Conv2d without using any of PyTorch's (or numpy's) pre-defined convolutional functions."]},{"cell_type":"code","metadata":{"id":"UM1pvb06OY38","colab_type":"code","colab":{}},"source":["def conv_forward_naive(x, w, b, conv_param):\n","    \"\"\"\n","    A naive Python implementation of a convolutional layer.\n","    The input consists of N data points, each with C channels, height H and\n","    width W. We convolve each input with F different filters, where each filter\n","    spans all C channels and has height HH and width WW.\n","    Input:\n","    - x: Input data of shape (N, C, H, W)\n","    - w: Filter weights of shape (F, C, HH, WW)\n","    - b: Biases, of shape (F,)\n","    - conv_param: A dictionary with the following keys:\n","      - 'stride': The number of pixels between adjacent receptive fields in the\n","        horizontal and vertical directions.\n","      - 'pad': The number of pixels that will be used to zero-pad the input. \n","        \n","    During padding, 'pad' zeros should be placed symmetrically (i.e., equally on both sides)\n","    along the height and width axes of the input. Be careful not to modfiy the original\n","    input x directly.\n","    Returns an array.\n","    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n","      H' = 1 + (H + 2 * pad - HH) / stride\n","      W' = 1 + (W + 2 * pad - WW) / stride\n","    \"\"\"\n","    out = None\n","\n","    N, C, H, W = x.shape\n","    num_filters, _, filter_height, filter_width = w.shape\n","    stride, pad = conv_param['stride'], conv_param['pad']\n","\n","    # Check dimensions.\n","    assert (W + 2 * pad - filter_width) % stride == 0, 'width does not work'\n","    assert (H + 2 * pad - filter_height) % stride == 0, 'height does not work'\n","\n","    ###########################################################################\n","    # TODO: Implement the forward pass of a convolutional layer without using #\n","    #       nn.Conv2D or other implementations of convolutions. Instead, use  #\n","    #       standard for- and while-loops to iterate over the tensors.        #\n","    #                                                                         #\n","    # Hint: you can use the function torch.nn.functional.pad for padding.     #\n","    ###########################################################################\n","\n","    H_1 = 1 + (H + 2 * pad - filter_height) // stride\n","    W_1 = 1 + (W + 2 * pad - filter_width) // stride\n","\n","    out = torch.zeros((N, num_filters, H_1, W_1))\n","    \n","    pad_x = torch.nn.functional.pad(x, pad=(pad, pad, pad, pad))\n","\n","    for n in range(N):\n","      x_slice = pad_x[n, :, :, :]\n","      for n_f in range(num_filters):\n","        for row in range(H_1):\n","          row_start = row*stride\n","          row_end = row_start + filter_height\n","          for col in range(W_1):\n","            col_start = col*stride\n","            col_end = col_start + filter_width\n","            out[n, n_f, row, col] = torch.sum(x_slice[:, row_start:row_end, col_start:col_end] * w[n_f]) + b[n_f]\n","        \n","    return out\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cO4BoyElOY4A","colab_type":"text"},"source":["You can test your implementation by running the following testing code:"]},{"cell_type":"code","metadata":{"id":"Re7-1eWNOY4B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"134233d0-cac0-4c54-c08c-fb6af90cbfc3","executionInfo":{"status":"ok","timestamp":1587609755282,"user_tz":0,"elapsed":1337,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhG5qWFeH1GnV295n1cnsZvw-k7ZfMGTjWoACTv=s64","userId":"02961330227233495639"}}},"source":["# Make convolution module.\n","w_shape = (3, 3, 4, 4)\n","w = torch.linspace(-0.2, 0.3, steps=torch.prod(torch.tensor(w_shape))).reshape(w_shape)\n","b = torch.linspace(-0.1, 0.2, steps=3)\n","\n","# Compute output of module and compare against reference values.\n","x_shape = (2, 3, 4, 4)\n","x = torch.linspace(-0.1, 0.5, steps=torch.prod(torch.tensor(x_shape))).reshape(x_shape)\n","out = conv_forward_naive(x, w, b, {'stride': 2, 'pad': 1})\n","\n","correct_out = torch.tensor([[[[-0.08759809, -0.10987781],\n","                              [-0.18387192, -0.2109216 ]],\n","                             [[ 0.21027089,  0.21661097],\n","                              [ 0.22847626,  0.23004637]],\n","                             [[ 0.50813986,  0.54309974],\n","                              [ 0.64082444,  0.67101435]]],\n","                            [[[-0.98053589, -1.03143541],\n","                              [-1.19128892, -1.24695841]],\n","                             [[ 0.69108355,  0.66880383],\n","                              [ 0.59480972,  0.56776003]],\n","                             [[ 2.36270298,  2.36904306],\n","                              [ 2.38090835,  2.38247847]]]])\n","\n","# Compare your output to ours; difference should be around e-8\n","print('Testing conv_forward_naive')\n","rel_error = ((out - correct_out) / (out + correct_out + 1e-6)).mean()\n","print('difference: ', rel_error)\n","if abs(rel_error) < 1e-6:\n","    print('Nice work! Your implementation of a convolution layer works correctly.')\n","else:\n","    print('Something is wrong. The output was expected to be {} but it was {}'.format(correct_out, out))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Testing conv_forward_naive\n","difference:  tensor(1.9275e-08)\n","Nice work! Your implementation of a convolution layer works correctly.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GjTq5-k_q8s_"},"source":["\n","We will now replace the logistic regressor from the last tutorial by a small convolutional network with two convolutional layers and a linear layer, and ReLU activations in between the layers. Implement the model and use the same functions as before to train and test the convolutional network."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5t4hWoUYpp05","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"d201c75c-ee4e-4ae5-fc17-43ea89d17a68","executionInfo":{"status":"ok","timestamp":1587609813828,"user_tz":0,"elapsed":59528,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhG5qWFeH1GnV295n1cnsZvw-k7ZfMGTjWoACTv=s64","userId":"02961330227233495639"}}},"source":["class ConvolutionalNetwork(nn.Module):\n","  \"\"\"Simple convolutional network.\"\"\"\n","  \n","  def __init__(self, image_side_size, num_classes, in_channels=1):\n","      super(ConvolutionalNetwork, self).__init__()\n","        \n","      # Fill these in:\n","      ##########################################################################\n","      # TODO: Implement a convulutional and a linear part.                     #\n","      # Hint: see forward() to understand how they should work together.       #\n","      ##########################################################################        \n","      self.conv_network1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=3)\n","      self.conv_network2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n","      self.relu = nn.ReLU()\n","      self.linear = nn.Linear(in_features=24*24*64, out_features=num_classes)\n","      \n","      \n","  def forward(self, x):\n","      x = self.conv_network1(x)\n","      x = self.relu(x)\n","      x = self.conv_network2(x)\n","      x = self.relu(x)\n","      x = self.linear(x.view(x.size(0), -1))\n","      return x\n","    \n","# Create and train convolutional network.\n","# The accuracy should be around 98%.\n","conv_model = ConvolutionalNetwork(28, 10)\n","###########################################################################\n","# TODO: Create criterion and optimize here.                               #\n","###########################################################################\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(conv_model.parameters(), lr=0.01, momentum=0.9)\n","\n","train(conv_model, criterion, train_loader, optimizer, num_epochs=5)\n","test(conv_model, test_loader)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Train Epoch: 0 \tLoss: 0.098756\n","Train Epoch: 1 \tLoss: 0.062366\n","Train Epoch: 2 \tLoss: 0.050000\n","Train Epoch: 3 \tLoss: 0.037200\n","Train Epoch: 4 \tLoss: 0.028672\n","Accuracy: 9824 / 10000 (98%)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["98.24"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"9RztZL6FOY4H","colab_type":"text"},"source":["Inspect the filters in the first layer of the trained convolutional network. What do they look like? Why?"]},{"cell_type":"code","metadata":{"id":"Be3_AT4aiVOH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"outputId":"63a128b3-4652-4fff-e2f4-352acee3239b","executionInfo":{"status":"ok","timestamp":1587609813829,"user_tz":0,"elapsed":58509,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhG5qWFeH1GnV295n1cnsZvw-k7ZfMGTjWoACTv=s64","userId":"02961330227233495639"}}},"source":["conv_model"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ConvolutionalNetwork(\n","  (conv_network1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (conv_network2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (relu): ReLU()\n","  (linear): Linear(in_features=36864, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LuSuQHT6bNdU","colab":{"base_uri":"https://localhost:8080/","height":248},"outputId":"862929a3-5ef6-4ceb-9292-c946a0bd5280","executionInfo":{"status":"ok","timestamp":1587609813829,"user_tz":0,"elapsed":57415,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhG5qWFeH1GnV295n1cnsZvw-k7ZfMGTjWoACTv=s64","userId":"02961330227233495639"}}},"source":["# first_conv = list(conv_model.conv_network1.children())[0]\n","first_conv = conv_model.conv_network1\n","show(torchvision.utils.make_grid(\n","    first_conv.weight,\n","    normalize=True,\n","    nrow=8,\n","))"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"71LRkRxndajG"},"source":["## Batch Normalization\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oN8qTuGkZ-3X"},"source":["Batch normalization is tenchique that allows to make training more stable fast [1].\n","\n","Below we define a convolutional network with 3 layers. Before each ReLU layer we insert a BatchNorm2d layer if `use_batch_norm` is `True`. This improves the convergence as guarantees as values have the same variance asn zero-means. As a result on average exactly half of the values will be nulled by ReLU.\n","\n","[1] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015).\n","\n","**Task**. Go ahead and add batch normalization layer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vg4CO_WDeLqh","colab":{}},"source":["class ConvolutionalNetworkWithBN(nn.Module):\n","  \"\"\"Convolutional network uses batch normalization when `use_batch_norm` is `True`.\"\"\"\n","  \n","  def __init__(self, use_batch_norm=False):\n","      super().__init__()\n","      num_channels = 8\n","      self.use_batch_norm = use_batch_norm\n","        \n","      # We define all our layers in a single Sequential. If use_batch_norm is \n","      # True, we'd like maybe_batch_norm_layer to produce a batchnorm layer.\n","      self.conv = nn.Sequential(\n","          # 1x28x28 -> 8x24x24.\n","          nn.Conv2d(1, num_channels, kernel_size=5),\n","          # 8x24x24 -> 8x12x12.\n","          nn.MaxPool2d(2),\n","          self.maybe_batch_norm_layer(num_channels),\n","          nn.ReLU(inplace=True),\n","          nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1),          \n","          self.maybe_batch_norm_layer(num_channels),            \n","          nn.ReLU(inplace=True),\n","          # 8x12x12 -> 8x8x8.\n","          nn.Conv2d(num_channels, num_channels, kernel_size=5),\n","          # 8x8x8 -> 8x4x4.\n","          nn.MaxPool2d(2),\n","          self.maybe_batch_norm_layer(num_channels),\n","          nn.ReLU(inplace=True),\n","      )\n","      self.linear = nn.Linear(num_channels * 4 ** 2, 10)  \n","      \n","  def maybe_batch_norm_layer(self, num_channels):\n","      if self.use_batch_norm:\n","          ###########################################################################\n","          # TODO: Add batch norm layer                                              #\n","          # Put your code between START_GRADING and END_GRADING.                    #\n","          ###########################################################################\n","          return nn.BatchNorm2d(num_features=num_channels, \n","                                eps=1e-05, \n","                                momentum=0.1, \n","                                affine=True, \n","                                track_running_stats=True)\n","      else:\n","          # This layer will copy its input to the output.\n","          return nn.Identity()\n","      \n","  def forward(self, x):\n","      x = self.conv(x)  \n","      x = self.linear(x.view(x.size(0), -1))\n","      return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qbtzzBhiJ4PW"},"source":["To see how batch normalization improves stability, let's try to train the neural network with different learning rates and check the accuracies."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YXuSliJF3r7u","colab":{"base_uri":"https://localhost:8080/","height":271},"outputId":"2cca81b0-e027-4433-f936-da6f7ba612b5","executionInfo":{"status":"ok","timestamp":1587609862482,"user_tz":0,"elapsed":100890,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhG5qWFeH1GnV295n1cnsZvw-k7ZfMGTjWoACTv=s64","userId":"02961330227233495639"}}},"source":["for lr in 0.3, 0.1, 0.03, 0.01, 0.003:\n","  conv_model = ConvolutionalNetworkWithBN()\n","  optimizer = torch.optim.SGD(conv_model.parameters(), lr=lr)\n","  train(conv_model, nn.CrossEntropyLoss(), train_loader, optimizer, num_epochs=1)\n","  accuracy = test(conv_model, test_loader)\n","  if accuracy > 96:\n","    print(f'##### lr={lr} leads to GOOD accuracy (no batch normalization)')\n","  elif accuracy > 90:\n","    print(f'##### lr={lr} leads to DECENT accuracy (no batch normalization)')\n","  else:\n","    print(f'##### lr={lr} leads to BAD accuracy (no batch normalization)')   "],"execution_count":12,"outputs":[{"output_type":"stream","text":["Train Epoch: 0 \tLoss: 0.134334\n","Accuracy: 9546 / 10000 (95%)\n","##### lr=0.3 leads to DECENT accuracy (no batch normalization)\n","Train Epoch: 0 \tLoss: 0.120836\n","Accuracy: 9564 / 10000 (96%)\n","##### lr=0.1 leads to DECENT accuracy (no batch normalization)\n","Train Epoch: 0 \tLoss: 0.211505\n","Accuracy: 9376 / 10000 (94%)\n","##### lr=0.03 leads to DECENT accuracy (no batch normalization)\n","Train Epoch: 0 \tLoss: 0.482942\n","Accuracy: 8808 / 10000 (88%)\n","##### lr=0.01 leads to BAD accuracy (no batch normalization)\n","Train Epoch: 0 \tLoss: 2.295210\n","Accuracy: 1298 / 10000 (13%)\n","##### lr=0.003 leads to BAD accuracy (no batch normalization)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WrJBxM03KOZ-"},"source":["As you can see learning rate should be in a narrow region to get GOOD result.\n","\n","Now let's try to run the same batch normalization. Before running the code you should finish TODO in ConvolutionalNetworkWithBN code. If your code is correct, then accuracies will improve."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6u2JJdnX5VKF","colab":{"base_uri":"https://localhost:8080/","height":271},"outputId":"1d17b39d-8cd1-4f80-91df-736bb5c90cb8","executionInfo":{"status":"ok","timestamp":1587609914001,"user_tz":0,"elapsed":151616,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhG5qWFeH1GnV295n1cnsZvw-k7ZfMGTjWoACTv=s64","userId":"02961330227233495639"}}},"source":["for lr in 0.3, 0.1, 0.03, 0.01, 0.003:\n","  conv_model = ConvolutionalNetworkWithBN(use_batch_norm=True)\n","  optimizer = torch.optim.SGD(conv_model.parameters(), lr=lr)\n","  train(conv_model, nn.CrossEntropyLoss(), train_loader, optimizer, num_epochs=1)\n","  accuracy = test(conv_model, test_loader)\n","  if accuracy > 96:\n","    print(f'##### lr={lr} leads to GOOD accuracy (with batch normalization)')\n","  elif accuracy > 90:\n","    print(f'##### lr={lr} leads to DECENT accuracy (with batch normalization)')\n","  else:\n","    print(f'##### lr={lr} leads to BAD accuracy (with batch normalization)')   \n","  if lr >= 0.01:\n","    assert accuracy > 90, 'Accuracy is too low. Check that your BatchNorm implementation is correct!'"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Train Epoch: 0 \tLoss: 0.064469\n","Accuracy: 9838 / 10000 (98%)\n","##### lr=0.3 leads to GOOD accuracy (with batch normalization)\n","Train Epoch: 0 \tLoss: 0.062516\n","Accuracy: 9785 / 10000 (98%)\n","##### lr=0.1 leads to GOOD accuracy (with batch normalization)\n","Train Epoch: 0 \tLoss: 0.093912\n","Accuracy: 9785 / 10000 (98%)\n","##### lr=0.03 leads to GOOD accuracy (with batch normalization)\n","Train Epoch: 0 \tLoss: 0.175680\n","Accuracy: 9613 / 10000 (96%)\n","##### lr=0.01 leads to GOOD accuracy (with batch normalization)\n","Train Epoch: 0 \tLoss: 0.532375\n","Accuracy: 9247 / 10000 (92%)\n","##### lr=0.003 leads to DECENT accuracy (with batch normalization)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gdgr7eNFTTAn"},"source":["**Question [optional]:** Replace the batch normalization layer in your network by your own implementation. Confirm your batch-normalization implementation is correct."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MAuhUMtjTj3f","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a6f09220-ea36-49b9-a0f5-d2acf0ee9837","executionInfo":{"status":"ok","timestamp":1587609914002,"user_tz":0,"elapsed":151302,"user":{"displayName":"Sewade Ogun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhG5qWFeH1GnV295n1cnsZvw-k7ZfMGTjWoACTv=s64","userId":"02961330227233495639"}}},"source":["class MyBatchNorm2d(nn.Module):\n","    \"\"\"Simple implementation of batch normalization.\"\"\"\n","\n","    def __init__(self, num_channels, momentum=0.1, epsilon=1e-5):\n","        super(MyBatchNorm2d, self).__init__()\n","\n","        # Initialize bias and gain parameters.\n","        self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n","        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n","\n","        # Initialize moving averages.\n","        self.epsilon = epsilon\n","        self.momentum = momentum\n","        self.register_buffer('running_mean', torch.zeros((1, num_channels, 1, 1)))\n","        self.register_buffer('running_var', torch.ones((1, num_channels, 1, 1)))\n","        self.num_channels = num_channels\n","        \n","    def forward(self, x):\n","        # Check that input is of correct size.\n","        assert x.dim() == 4, 'input should be NCHW'\n","        assert x.size(1) == self.gamma.numel()\n","        \n","        ########################################################################\n","        # TODO: Add batch norm layer implementation.                           #\n","        # You code should:                                                     #\n","        #   * Compute mean and var.                                            #\n","        #   * Update running_mean and running_var.                             #\n","        #   * Apply mean - variance normalization to x.                        #\n","        # Put your code between START_GRADING and END_GRADING.                 #\n","        ########################################################################\n","\n","        if self.training:\n","          mean = x.mean([0, 2, 3])\n","          var = x.var([0, 2, 3])\n","\n","          mean = mean.view(1, -1, 1, 1)\n","          var = var.view(1, -1, 1, 1)\n","\n","          # update running mean and var\n","          with torch.no_grad():\n","            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum*mean\n","            self.running_var = (1 - self.momentum) * self.running_var + self.momentum*var\n","\n","          x = (x - mean)/(torch.sqrt(var) + self.epsilon)\n","        else:\n","          x = (x - self.running_mean)/(torch.sqrt(self.running_var) + self.epsilon)\n","\n","        return self.gamma*x + self.beta\n","\n","# Use this code to test if your implementation is correct.\n","batch_size, num_channels, im_size = 32, 8, 6\n","batchnorm1 = nn.BatchNorm2d(num_channels)\n","batchnorm2 = MyBatchNorm2d(num_channels)\n","for key, param in batchnorm1.named_parameters():\n","    if key == 'weight':\n","        param.data.fill_(1.0)  # undo random initialization in nn.BatchNorm2d\n","for mode in [True, False]:     # test in training and evaluation mode\n","    batchnorm1.train(mode=mode)\n","    batchnorm2.train(mode=mode)\n","    for _ in range(5):\n","        x = torch.randn(batch_size, num_channels, im_size, im_size) + 10.0\n","        out1 = batchnorm1(x)\n","        out2 = batchnorm2(x)\n","        #print(batchnorm1.running_mean.shape, batchnorm2.running_mean.shape)\n","        assert (batchnorm1.running_mean - batchnorm2.running_mean.squeeze()).abs().max() < 1e-5, \\\n","            'running mean is incorrect (%s mode)' % ('train' if mode else 'eval')\n","        assert (batchnorm1.running_var - batchnorm2.running_var.squeeze()).abs().max() < 1e-5, \\\n","            'running variance is incorrect (%s mode)' % ('train' if mode else 'eval')\n","        assert (out1 - out2).abs().max() < 5e-3, \\\n","            'normalized output is incorrect (%s mode)' % ('train' if mode else 'eval')\n","print('All OK!')\n","        "],"execution_count":14,"outputs":[{"output_type":"stream","text":["All OK!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K_mlTK4LMlse","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}